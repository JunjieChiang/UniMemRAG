{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee20f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.pop(\"http_proxy\", None)\n",
    "os.environ.pop(\"https_proxy\", None)\n",
    "os.environ[\"NO_PROXY\"] = \"localhost,127.0.0.1\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from config import Config\n",
    "from unimemrag.embedding.models.QwenTextEmbedding import QwenTextEmbedding\n",
    "from unimemrag.retriever import ClipEmbedding\n",
    "from unimemrag.memory_forest.memory_forest import MemoryForestStore\n",
    "\n",
    "cfg = Config(collection=\"memtree\")                          \n",
    "embed_model = ClipEmbedding(model_name=\"../../ckpts/clip-vit-large-patch14\", device_map=\"balanced\", image_processor_name_or_path=\"../../ckpts/clip-vit-large-patch14\")\n",
    "text_embedder = QwenTextEmbedding(model_name=\"../../ckpts/Qwen3-Embedding-0.6B\", model_kwargs={'device_map': \"balanced\"})\n",
    "memforest_store = MemoryForestStore(cfg, vector_size=embed_model.dim, leaf_text_vector_size=text_embedder.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e388faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "replace all images in existing trees with cached versions\n",
    "'''\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from sklearn import tree\n",
    "os.environ.pop(\"http_proxy\", None)\n",
    "os.environ.pop(\"https_proxy\", None)\n",
    "os.environ[\"NO_PROXY\"] = \"localhost,127.0.0.1\"\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dataclasses import asdict\n",
    "from itertools import islice\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from unimemrag.memory_forest.memory_forest import build_tree, iter_wiki_dict\n",
    "from unimemrag.utils.image_cache import download_images_for_kb, load_image_cache, replace_payload_image_urls, save_image_cache\n",
    "\n",
    "\n",
    "KB_PATH = Path(\"trees_all.json\")\n",
    "\n",
    "with KB_PATH.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "    KB = json.load(fh)\n",
    "\n",
    "IMAGE_CACHE_DIR = Path('../../benchmark/infoseek/images_100k')\n",
    "IMAGE_CACHE_INDEX = IMAGE_CACHE_DIR / \"image_cache_index.json\"\n",
    "image_cache = load_image_cache(IMAGE_CACHE_INDEX)\n",
    "\n",
    "if not image_cache:\n",
    "    image_cache = download_images_for_kb(KB, IMAGE_CACHE_DIR, max_workers=64, resume=True)\n",
    "    save_image_cache(image_cache, IMAGE_CACHE_INDEX)\n",
    "else:\n",
    "    print(f\"Loaded {len(image_cache)} cached entries from {IMAGE_CACHE_INDEX}\")\n",
    "\n",
    "def localize_payload(payload):\n",
    "    return replace_payload_image_urls(dict(payload), image_cache, used_for_tree=True)\n",
    "\n",
    "trees = []\n",
    "# for wiki_url, payload in iterator:\n",
    "for payload in tqdm(KB, desc=\"Localizing payloads\"):\n",
    "    payload = localize_payload(payload)\n",
    "    # tree = build_tree(wiki_url, payload)\n",
    "    # tree = build_tree(\n",
    "    #    wiki_url,\n",
    "    #    payload,\n",
    "    #    llm=llm,\n",
    "    #    llm_model=\"qwen3-8b\",\n",
    "    #    max_summary_sections=None,\n",
    "    #    llm_workers=10,\n",
    "    #    llm_request_kwargs={\"max_tokens\": 512, \"temperature\": 0.7, \"extra_body\": {\"enable_thinking\": False}}\n",
    "    #    )      \n",
    "    trees.append(payload)\n",
    "\n",
    "TREES_OUT = Path(\"trees_all_new.json\")\n",
    "with TREES_OUT.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "    json.dump(trees, fh, ensure_ascii=False, indent=2)\n",
    "print(f\"Saved {len(trees)} trees to {TREES_OUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4405b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "encyclopedic-vqa\n",
    "'''\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from sklearn import tree\n",
    "os.environ.pop(\"http_proxy\", None)\n",
    "os.environ.pop(\"https_proxy\", None)\n",
    "os.environ[\"NO_PROXY\"] = \"localhost,127.0.0.1\"\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dataclasses import asdict\n",
    "from itertools import islice\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from unimemrag.memory_forest.memory_forest import build_tree, iter_wiki_dict\n",
    "\n",
    "# llm = OpenAI(\n",
    "#        api_key=\"sk-d6de5f2c6e4c458e9b53e9d96a5e75bb\",\n",
    "#        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "# )\n",
    "\n",
    "# KB_PATH = Path(\"../../benchmark/encyclopedic_vqa/encyclopedic_kb_wiki.json\")\n",
    "KB_PATH = Path(\"trees_evqa.json\")\n",
    "\n",
    "with KB_PATH.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "    KB = json.load(fh)\n",
    "\n",
    "# IMAGE_CACHE_DIR = Path('../../benchmark/infoseek/subset/wiki_text/images_5k')\n",
    "# IMAGE_CACHE_INDEX = IMAGE_CACHE_DIR / \"image_cache_index.json\"\n",
    "# image_cache = load_image_cache(IMAGE_CACHE_INDEX)\n",
    "\n",
    "# if not image_cache:\n",
    "#     image_cache = download_images_for_kb(KB, IMAGE_CACHE_DIR, max_workers=64, resume=True)\n",
    "#     save_image_cache(image_cache, IMAGE_CACHE_INDEX)\n",
    "# else:\n",
    "#     print(f\"Loaded {len(image_cache)} cached entries from {IMAGE_CACHE_INDEX}\")\n",
    "\n",
    "# def localize_payload(payload):\n",
    "#     return replace_payload_image_urls(dict(payload), image_cache)\n",
    "\n",
    "\n",
    "total = 1\n",
    "iterator = islice(iter_wiki_dict(KB), total)\n",
    "iterator = tqdm(iterator, total=total, desc=\"Building Trees\")\n",
    "\n",
    "trees = []\n",
    "for wiki_url, payload in iterator:\n",
    "    payload = localize_payload(payload)\n",
    "    # tree = build_tree(wiki_url, payload)\n",
    "    # tree = build_tree(\n",
    "    #    wiki_url,\n",
    "    #    payload,\n",
    "    #    llm=None,\n",
    "    #    # llm_model=\"qwen3-8b\",\n",
    "    #    # max_summary_sections=None,\n",
    "    #    # llm_workers=10,\n",
    "    #    # llm_request_kwargs={\"max_tokens\": 512, \"temperature\": 0.7, \"extra_body\": {\"enable_thinking\": False}}\n",
    "    #    show_progress=False,\n",
    "    #    )      \n",
    "    # trees.append(tree)\n",
    "\n",
    "# TREES_OUT = Path(\"trees_evqa.json\")\n",
    "# with TREES_OUT.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "#     json.dump([asdict(t) for t in trees], fh, indent=2)\n",
    "# print(f\"Saved {len(trees)} trees to {TREES_OUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5bab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "load trees from file\n",
    "'''\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from unimemrag.memory_forest.memory_forest import MemoryTree, RootNode, EventNode, LeafNode\n",
    "\n",
    "TREES_OUT = Path(\"trees_infoseek_new.json\")\n",
    "\n",
    "with TREES_OUT.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "    data = json.load(fh)\n",
    "\n",
    "def tree_from_dict(d):\n",
    "    root = RootNode(**d[\"root\"])\n",
    "    events = [EventNode(**e) for e in d.get(\"events\", [])]\n",
    "    leaves = [LeafNode(**l) for l in d.get(\"leaves\", [])]\n",
    "    return MemoryTree(tree_id=d[\"tree_id\"], root=root, events=events, leaves=leaves)\n",
    "\n",
    "trees = [tree_from_dict(x) for x in data]\n",
    "print(len(trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8a0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trees[8].events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deebb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入库：多模态 leaf\n",
    "memforest_store.ingest_trees_new(\n",
    "    trees,\n",
    "    embed_model,\n",
    "    leaf_text_embedder=text_embedder,\n",
    "    leaf_text_workers=1,\n",
    "    beta=0.4,\n",
    "    batch_size=16,        # mm_batch_size\n",
    "    text_batch_size=8,    # text-only embedding batch\n",
    "    text_workers=1,\n",
    "    image_workers=8,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d5d247c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mobuser/miniconda3/envs/unimemrag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/mobuser/miniconda3/envs/unimemrag/lib/python3.11/site-packages/accelerate/utils/modeling.py:1598: UserWarning: The following device_map keys do not match any submodules in the model: ['logit_scale']\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "/home/mobuser/jjj/UniMemRAG/examples/../unimemrag/vector_store/qdrant.py:40: UserWarning: Qdrant client version 1.14.3 is incompatible with server version 1.16.3. Major versions should match and minor version difference must not exceed 1. Set check_compatibility=False to skip version check.\n",
      "  self.client = QdrantClient(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loading tree index from memforest store\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "os.environ.pop(\"http_proxy\", None)\n",
    "os.environ.pop(\"https_proxy\", None)\n",
    "os.environ[\"NO_PROXY\"] = \"localhost,127.0.0.1\"\n",
    "\n",
    "from config import Config\n",
    "from unimemrag.embedding.models.ClipEmbedding import ClipEmbedding\n",
    "from unimemrag.embedding.models.QwenTextEmbedding import QwenTextEmbedding\n",
    "from unimemrag.memory_forest import MemoryForestStore\n",
    "\n",
    "cfg = Config(collection=\"memtree\")\n",
    "embed_model = ClipEmbedding(model_name=\"../../ckpts/clip-vit-large-patch14\", device_map=\"balanced\")\n",
    "text_embedder = QwenTextEmbedding(model_name=\"../../ckpts/Qwen3-Embedding-0.6B\", model_kwargs={'device_map': \"balanced\"})\n",
    "memforest_store = MemoryForestStore(cfg, vector_size=embed_model.dim, leaf_text_vector_size=text_embedder.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5d247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检索：collapsed retrieval\n",
    "results = memforest_store.collapsed_retrieve(\n",
    "    embed_model,\n",
    "    alpha=0.9,\n",
    "    query_text=\"What is the length of the wingspan in millimetre of Echinargus isola\",\n",
    "    query_image=\"../../benchmark/oven/05/oven_05036901\",\n",
    "    # query_image=\"https://oregonflora.org/dbimages/OFPimages/OFPImages_big/1522/DIG20072.jpg\",\n",
    "    leaf_text_embedder=text_embedder,\n",
    "    leaf_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8838fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results[1].leaves\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c1ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tree in results:\n",
    "    print(\"=== Root:\", tree.root.payload.get(\"topic\"))\n",
    "    for event in tree.events:\n",
    "        sec_id = event.id\n",
    "        meta = event.payload.get(\"metadata\", {})\n",
    "        title = meta.get(\"section_title\") or event.payload.get(\"summary\")\n",
    "        section_chunks = [\n",
    "            leaf.payload[\"content\"]\n",
    "            for leaf in tree.leaves.get(sec_id, [])\n",
    "            if \"content\" in leaf.payload\n",
    "        ]\n",
    "        section_text = \"\\n\".join(section_chunks)\n",
    "        print(f\"\\nSection: {title}\")\n",
    "        print(\"Images:\", meta.get(\"section_images\", []))\n",
    "        print(\"Text:\", section_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a39579",
   "metadata": {},
   "outputs": [],
   "source": [
    "memforest_store.client.delete_collection(memforest_store.cfg.collection)\n",
    "memforest_store.client.delete_collection(memforest_store.event_collection)\n",
    "memforest_store.client.delete_collection(memforest_store.leaf_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b59b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "memforest_store.client.count(memforest_store.roof_collection, exact=True)\n",
    "# memforest_store.client.count(memforest_store.leaf_collection, exact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7016de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unimemrag.vlm.QwenVL import QwenVL\n",
    "vlm = QwenVL(\n",
    "    model_path=\"../../ckpts/Qwen2.5-VL-7B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4829b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = results[0]\n",
    "\n",
    "def format_context(tree_result, top_sections=3, max_chars=1024):\n",
    "    root = tree_result.root\n",
    "    meta = root.payload.get(\"metadata\", {}) or {}\n",
    "    lines = [\n",
    "        f\"Topic: {root.payload.get('topic') or meta.get('source_url', 'n/a')}\",\n",
    "        f\"Tree ID: {tree_result.tree_id}\",\n",
    "        f\"Alignment score: {meta.get('alignment_best_score', 'n/a')}\",\n",
    "        \"\",\n",
    "    ]\n",
    "    for event in tree_result.events[:top_sections]:\n",
    "        emeta = event.payload.get(\"metadata\", {}) or {}\n",
    "        title = emeta.get(\"section_title\") or event.payload.get(\"summary\") or \"Unknown section\"\n",
    "        lines.append(f\"Section: {title}\")\n",
    "        section_preview = (emeta.get(\"section_preview\") or event.payload.get(\"summary\") or \"\").strip()\n",
    "        if section_preview:\n",
    "            lines.append(section_preview)\n",
    "        leaf_hits = tree_result.leaves.get(event.id, [])\n",
    "        for idx, leaf_hit in enumerate(leaf_hits[:2], start=1):\n",
    "            snippet = (leaf_hit.payload.get(\"content\") or \"\").strip()\n",
    "            if not snippet:\n",
    "                continue\n",
    "            lines.append(f\"Paragraph {idx}: {snippet}\")\n",
    "        lines.append(\"\")\n",
    "    context = \"\\n\".join(lines).strip()\n",
    "    if max_chars and max_chars > 0 and len(context) > max_chars:\n",
    "        truncated = context[:max_chars]\n",
    "        if \"\\n\" in truncated:\n",
    "            truncated = truncated.rsplit(\"\\n\", 1)[0]\n",
    "        context = truncated\n",
    "    return context\n",
    "\n",
    "context = format_context(result, top_sections=3)\n",
    "print(\"formated_context:\", context)\n",
    "question=\"Who built this object that has not cash dispensing features?\",\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"You are a helpful assistant. Please answer the question based on the context provided.\"}\n",
    "    ]},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\", \"image\": \"../atm-machine.jpg\"},\n",
    "        {\"type\": \"text\",  \"text\": f\"Here's the contexts:\\n{context}\\n\\nNow, answer the question:\\n{question}\"}\n",
    "    ]}\n",
    "]\n",
    "print(messages)\n",
    "\n",
    "answer = vlm.chat(messages, max_new_tokens=32768, temperature=0.7)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d15b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from infoseek_image_utils import resolve_images_root\n",
    "\n",
    "# image_file = resolve_images_root(\"oven_05031362\")\n",
    "image_file = \"../../benchmark/oven/08/oven_05012059.jpg\"\n",
    "\n",
    "# print(image_file)\n",
    "img = Image.open(image_file)\n",
    "img.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unimemrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
